# Project Description

## Overview
Currently, our customers and users are manually interpreting written text from various sources like course evaluations, peer reviews, twitter mentions, etc.  The problem is that manual interpretation is slow, unquantified, and prone to discrepencies in understanding between readers.  Our goal is provide automatic, programmatic, and quantifiable intepretations of written text.  The outcome is to provide our customers/users with better understanding, a stronger analytic toolset, and the ability to provide better service in their various roles.

We want to make better, more responsive, facualty, departments, and universities.

## What is the customer's name?
- ??? Not sure about names yet, but they belong to these groups:
- Departments...
- Faculty members that have course evals
- Marketing department that does student surverys
- Marketing dep. for twitter data

## Customers' objectives
- Provide better service
- Understand their customers better
- Create more structure in their approach to feedback

## Who else does the customer consider to be a major stakeholder?
- For deps. -> the faculty and students
- For marketing -> Exec. leader (provost office?) and students

## Identify the users
- Faculty
- Marketing dep. staff
- Lab researchers
- Anyone that's interested in quantifying written text

## Typical background and capability
- Faculty (basic office skills, some are highly educated (Bachelor+))
- Marketing dep. staff (basic office skills, some are educated (Bachelor+))
- Lab researchers (data analytics skills/ research skills, expect most are educated (Bachelor+))

## What do the users do?
- **Faculty:**  Teach, try to provide a good experience, and try to improve services
- **Marketing Dep. Staff:** Advertise, promote, and conduct customer feedback
- **Lab researchers:**  Research, learn from data, offer system improvements

## List Assumptions made and describe how they are verified
- Assuming they care to improve the service
- Assuming they want to quantify written comments, reviews, etc.
- Assuming quantifiable data is more valuable than unquantifiable, human-interpreted, information

## List assumptions still outstanding
- All

## When and how will they be verified?
- Assumptions will be verified during interviews with customers, stakeholders, SMEs, etc.

## What is the impact on the system if they are invalid?
- Highly impactful, product would not be usuable/usable if our assumptions are wrong.

## Identify major risk areas (Technical and non-technical)
- Accuracy of the Sentiment Analysis
- Bad AI Modeling
- Product may not be valuable, useful, usable for customers/users

## Leading Edge Technology
- Natural Language Processing (NLP) and Sentiment Analysis
- Artifical Intelligence (AI)

## Schedule constraints
- We only have 3.5 months
- Scheduling interviews takes time.  We need schedule alignment for involved parties.

## Resource constraints
- 300 queries per months for MonkeyLearn
- 100s of cpu time per day for pythonanywhere server
- 512MB of file storage on pythonanywhere server
- Low bandwidth for pythonanywhere server

## Lack of customer feedback
- Yes, we might have low customer feedback

## Initial Project Timeline
-> ???

## Milestones and deliverables
-> ???  This will be based on assignments.